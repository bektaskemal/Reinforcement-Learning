{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTION OF CARTPOLE PROBLEM WITH TEMPORAL DIFFERENCE METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-15 17:50:26,031] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretization(observation):\n",
    "    \n",
    "    discrete=np.zeros((1,2))\n",
    "    theta_bins=np.linspace(-0.42, 0.42, 20)\n",
    "    thetadot_bins=np.linspace(-1,1, 10)\n",
    "    discrete[0][0]=np.digitize(observation[2],theta_bins)\n",
    "    discrete[0][1]=np.digitize(observation[3],thetadot_bins)\n",
    "    \n",
    "    return discrete.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A_probs= np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation[0][0]][observation[0][1]])\n",
    "        A_probs[best_action] += (1.0 - epsilon)\n",
    "        return A_probs#.reshape(1,-1)\n",
    "    \n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_games(Q,num_episodes) :\n",
    "    scores=[]\n",
    "    observations=[]\n",
    "    for episode in range(num_episodes):\n",
    "        score=0\n",
    "        observation=env.reset()\n",
    "        for t in range(200):\n",
    "            policy = make_epsilon_greedy_policy(Q, 0, env.action_space.n)\n",
    "            env.render()\n",
    "            state=discretization(observation)\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            observations.append(observation)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "    print('Average Score:',sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA : On-policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses the action selected by the policy for the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA Prediction update rule:\n",
    "    $$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha\\big[R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, num_episodes, discount_factor=0.9, alpha=0.5, epsilon=0.1):\n",
    "    \n",
    "    # Q table initialization\n",
    "    Q_sarsa = np.zeros((21,11, env.action_space.n))\n",
    "\n",
    "    policy = make_epsilon_greedy_policy(Q_sarsa, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        observation = env.reset()\n",
    "        state=discretization(observation)\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            next_state=discretization(next_observation)\n",
    "            \n",
    "            # Sarsa Update\n",
    "            next_action_probs = policy(next_state)\n",
    "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "            td_target = reward + discount_factor * Q_sarsa[next_state[0][0]][next_state[0][1]][next_action]\n",
    "            td_delta = td_target - Q_sarsa[state[0][0]][state[0][1]][action]\n",
    "            Q_sarsa[state[0][0]][state[0][1]][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "    return Q_sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q ÖĞRENMESİ : Off-Policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses best possible next action for the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning Prediction Update Rule:\n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha\\big[R_{t+1} + \\gamma max_a Q(S_{t+1},a) - Q(S_t,A_t)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, num_episodes, discount_factor=0.9, alpha=0.5, epsilon=0.1):\n",
    "    \n",
    "    \n",
    "    # Q table initialization\n",
    "    Q = np.zeros((21,11, env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        observation = env.reset()\n",
    "        state=discretization(observation)\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            next_state=discretization(next_observation)\n",
    "            \n",
    "            # Q Learning Update\n",
    "            best_next_action = np.argmax(Q[next_state[0][0]][next_state[0][1]])\n",
    "            td_target = reward + discount_factor * Q[next_state[0][0]][next_state[0][1]][best_next_action]\n",
    "            td_delta = td_target - Q[state[0][0]][state[0][1]][action]\n",
    "            Q[state[0][0]][state[0][1]][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected SARSA Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses expected value of possible next actions for the update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected SARSA Prediction update rule: \n",
    "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha\\big[R_{t+1} + \\gamma \\sum\\limits_a{\\pi(a\\mid S_{t+1})Q(S_{t+1},a)} - Q(S_t,A_t)\\big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Expected_Sarsa(env, num_episodes, discount_factor=1, alpha=0.5, epsilon=0.1):\n",
    "    \n",
    "    # Q Table Initialization\n",
    "    Q_expsarsa = np.zeros((21,11, env.action_space.n))\n",
    "    \n",
    "    policy = make_epsilon_greedy_policy(Q_expsarsa, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        state=discretization(observation)\n",
    "\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_observation, reward, done, _ = env.step(action)\n",
    "            next_state=discretization(next_observation)\n",
    "            \n",
    "            # Expected Sarsa Update\n",
    "            next_action_prob=policy(next_state)\n",
    "            expected_value = np.sum(np.multiply(Q_expsarsa[next_state[0][0]][next_state[0][1]][:],next_action_prob))\n",
    "            td_target = reward + discount_factor * expected_value\n",
    "            td_delta = td_target - Q_expsarsa[state[0][0]][state[0][1]][action]\n",
    "            Q_expsarsa[state[0][0]][state[0][1]][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "    return Q_expsarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_s = Sarsa(env, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_q= Q_learning(env, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q_es = Expected_Sarsa(env, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and the Render of the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 200.0\n"
     ]
    }
   ],
   "source": [
    "render_games(Q_s,5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 162.8\n"
     ]
    }
   ],
   "source": [
    "render_games(Q_q,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 200.0\n"
     ]
    }
   ],
   "source": [
    "render_games(Q_es,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
